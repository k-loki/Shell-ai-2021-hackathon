{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor \n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import StandardScaler,PowerTransformer\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from joblib import dump,load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "random_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'data\\train.csv'\n",
    "# load data\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "def preprocess(df,mode = 'train'):\n",
    "  frac = 0.1\n",
    "  if mode=='train':\n",
    "    # fill missing data\n",
    "    df['Total Cloud Cover [%]'].replace(-7999,np.nan,inplace = True)\n",
    "    df['Total Cloud Cover [%]'].replace(-6999,np.nan,inplace = True)\n",
    "    df['Total Cloud Cover [%]'].interpolate(limit = 10,limit_direction = 'both',inplace = True)  \n",
    "\n",
    "    #  create targets\n",
    "    df['t_30'] = df.groupby('DATE (MM/DD)')['Total Cloud Cover [%]'].shift(periods = -30,fill_value = -1)\n",
    "    df['t_60'] = df.groupby('DATE (MM/DD)')['Total Cloud Cover [%]'].shift(periods = -60,fill_value = -1)\n",
    "    df['t_90'] = df.groupby('DATE (MM/DD)')['Total Cloud Cover [%]'].shift(periods = -90,fill_value = -1)\n",
    "    df['t_120'] = df.groupby('DATE (MM/DD)')['Total Cloud Cover [%]'].shift(periods = -120,fill_value = -1)\n",
    "\n",
    "    cond = (df['Total Cloud Cover [%]'] == -1)\n",
    "    req_samples = df[cond].sample(frac = frac,random_state = random_seed)\n",
    "    not_req_samples = df[cond].drop(req_samples.index)\n",
    "    df.drop(not_req_samples.index,inplace=True)\n",
    "    \n",
    "    # selected fts---> []\n",
    "    # drop unwanted features\n",
    "    df.drop([\n",
    "            'DATE (MM/DD)',\n",
    "            'MST',\n",
    "            'Direct sNIP [W/m^2]',                # this feature is highly correlated with cmp22\n",
    "            'Tower Wet Bulb Temp [deg C]',        # highly correlated with other temperature readings\n",
    "            'Tower Dew Point Temp [deg C]',\n",
    "            'Snow Depth [cm]',\n",
    "            'Moisture',\n",
    "            'Albedo (CMP11)',\n",
    "            'Precipitation (Accumulated) [mm]',\n",
    "            'Azimuth Angle [degrees]'\n",
    "    ],axis =1,inplace = True)\n",
    "\n",
    "    return df\n",
    "  if mode == 'test':\n",
    "    df['Total Cloud Cover [%]'].replace(-7999,np.nan,inplace = True)\n",
    "    df['Total Cloud Cover [%]'].replace(-6999,np.nan,inplace = True)\n",
    "    df['Total Cloud Cover [%]'].interpolate(limit = 10,limit_direction = 'both',inplace = True)  \n",
    "\n",
    "    df.drop(columns={\n",
    "      'Time [Mins]',\n",
    "      'Direct sNIP [W/m^2]',                # this feature is highly correlated with cmp22\n",
    "      'Tower Wet Bulb Temp [deg C]',        # highly correlated with other temperature readings\n",
    "      'Tower Dew Point Temp [deg C]',\n",
    "      'Snow Depth [cm]',\n",
    "      'Moisture',\n",
    "      'Albedo (CMP11)',\n",
    "      'Precipitation (Accumulated) [mm]',\n",
    "      'Azimuth Angle [degrees]',\n",
    "      'scenario_set' \n",
    "    },inplace = True)\n",
    "    return df.iloc[-1,]\n",
    "\n",
    "df = preprocess(df,mode='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split(x,y,train_size=0.70):\n",
    "  return train_test_split(x,y,train_size=train_size,random_state=random_seed)\n",
    "\n",
    "X_train_30,X_test_30,Y_train_30,Y_test_30 = split(df.iloc[:,:-4].values,df['t_30'].values)\n",
    "X_train_60,X_test_60,Y_train_60,Y_test_60 = split(df.iloc[:,:-3].values,df['t_60'].values)\n",
    "X_train_90,X_test_90,Y_train_90,Y_test_90 = split(df.iloc[:,:-2].values,df['t_90'].values)\n",
    "X_train_120,X_test_120,Y_train_120,Y_test_120 = split(df.iloc[:,:-1].values,df['t_120'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': 1,\n",
    "    'dual': True,\n",
    "    'epsilon': 0.1,\n",
    "    'fit_intercept': True,\n",
    "    'loss': 'epsilon_insensitive',\n",
    "    'random_state': 42\n",
    "        }\n",
    "\n",
    "pipeline_30 = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('transformer',PowerTransformer()),\n",
    "    ('sgd',LinearSVR(**params))\n",
    "])\n",
    "\n",
    "pipeline_60 = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('transformer',PowerTransformer()),\n",
    "    ('sgd',LinearSVR(**params))\n",
    "])\n",
    "\n",
    "pipeline_90 = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('transformer',PowerTransformer()),\n",
    "    ('sgd',LinearSVR(**params))\n",
    "])\n",
    "\n",
    "pipeline_120 = Pipeline([\n",
    "    ('scaler',StandardScaler()),\n",
    "    ('transformer',PowerTransformer()),\n",
    "    ('sgd',LinearSVR(**params))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_30 = load(r'LinearSVR\\new_LinearSVRmodel_30.joblib')\n",
    "# pipeline_60 = load(r'LinearSVR\\new_LinearSVRmodel_60.joblib')\n",
    "# pipeline_90 = load(r'LinearSVR\\new_LinearSVRmodel_90.joblib')\n",
    "# pipeline_120 = load(r'LinearSVR\\new_LinearSVRmodel_120.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'n_estimators' : 500,\n",
    "    'max_depth' : 500,\n",
    "    'random_state':random_seed,\n",
    "    'num_leaves':60\n",
    "}\n",
    "\n",
    "lgb_30 = LGBMRegressor(**lgb_params)\n",
    "lgb_60 = LGBMRegressor(**lgb_params)\n",
    "lgb_90 = LGBMRegressor(**lgb_params)\n",
    "lgb_120 = LGBMRegressor(**lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_30 = VotingRegressor([('lsvr_30', pipeline_30), ('lgb_30', lgb_30)])\n",
    "model_60 = VotingRegressor([('lsvr_60', pipeline_60), ('lgb_60', lgb_60)])\n",
    "model_90 = VotingRegressor([('lsvr_90', pipeline_90), ('lgb_90', lgb_90)])\n",
    "model_120 = VotingRegressor([('lsvr_120',pipeline_120), ('lgb_120', lgb_120)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model,x_tr,x_ts,y_tr,y_ts):\n",
    "  model.fit(x_tr,y_tr)\n",
    "  preds = model.predict(x_ts)\n",
    "  print(model.score(x_ts,y_ts))\n",
    "  print(f\"mae score: {mae(y_ts,preds)}\")\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.873476319995545\n",
      "mae score: 8.040225855719612\n",
      "0.8831877203270491\n",
      "mae score: 7.600077843828635\n",
      "0.8887363839923138\n",
      "mae score: 7.400704962243625\n",
      "0.8885908540177627\n",
      "mae score: 7.350021842519031\n"
     ]
    }
   ],
   "source": [
    "pred_30 = train_and_validate(model_30,X_train_30,X_test_30,Y_train_30,Y_test_30)\n",
    "pred_60 = train_and_validate(model_60,X_train_60,X_test_60,Y_train_60,Y_test_60)\n",
    "pred_90 = train_and_validate(model_90,X_train_90,X_test_90,Y_train_90,Y_test_90)\n",
    "pred_120 = train_and_validate(model_120,X_train_120,X_test_120,Y_train_120,Y_test_120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   30.4s remaining:   45.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   31.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_30:[-10.57141988 -11.0783422  -11.79991664 -10.38384739  -8.54953601] ,mean---------> -10.476612425443701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   27.5s remaining:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   29.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_60:[ -9.69485112 -10.4175476  -11.32968203  -9.89830408  -7.66627878] ,mean---------> -9.801332723181336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   31.6s remaining:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   32.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_90:[ -9.22395403 -10.39519477 -11.39379841  -9.96785403  -7.41326992] ,mean---------> -9.678814231570067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   32.0s remaining:   48.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   32.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_120:[ -8.84545155 -10.15009291 -11.3586893   -9.9183558   -7.2088214 ] ,mean---------> -9.496282190693952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = r'data\\train.csv'\n",
    "# load data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "test_df = preprocess(df,mode='train')\n",
    "\n",
    "X_30,Y_30 = test_df.iloc[:,:-4].values,test_df['t_30'].values\n",
    "c_30 = cross_val_score(model_30,X_30,Y_30,scoring='neg_mean_absolute_error',verbose=1,n_jobs=-1)\n",
    "print(f\"c_30:{c_30} ,mean---------> {c_30.mean()}\")\n",
    "\n",
    "X_60,Y_60 = test_df.iloc[:,:-3].values,test_df['t_60'].values\n",
    "c_60 = cross_val_score(model_60,X_60,Y_60,scoring='neg_mean_absolute_error',verbose=1,n_jobs=-1)\n",
    "print(f\"c_60:{c_60 } ,mean---------> { c_60.mean()}\")\n",
    "\n",
    "X_90,Y_90 = test_df.iloc[:,:-2].values,test_df['t_90'].values\n",
    "c_90 = cross_val_score(model_90,X_90,Y_90,scoring='neg_mean_absolute_error',verbose=1,n_jobs=-1)\n",
    "print(f\"c_90:{c_90 } ,mean---------> { c_90.mean()}\")\n",
    "\n",
    "X_120,Y_120 = test_df.iloc[:,:-1].values,test_df['t_120'].values\n",
    "c_120 = cross_val_score(model_120,X_120,Y_120,scoring='neg_mean_absolute_error',verbose=1,n_jobs=-1)\n",
    "print(f\"c_120:{c_120} ,mean---------> {c_120.mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wd_test = pd.read_csv(r'data\\shell_test.csv')\n",
    "# test = pd.read_csv(r'data\\test.csv')\n",
    "\n",
    "# file_count = 0\n",
    "# for set in range(1,301):\n",
    "#   wd = wd_test[wd_test['scenario_set'] == set]\n",
    "#   # preprocessing test data\n",
    "#   last_sample = preprocess(wd,mode='test')\n",
    "\n",
    "#   # predicting test samples\n",
    "#   pred_30 = model_30.predict(last_sample.values.reshape(1,-1))\n",
    "#   last_sample['t_30'] = pred_30.item()\n",
    "#   pred_60 = model_60.predict(last_sample.values.reshape(1,-1))\n",
    "#   last_sample['t_60'] = pred_60.item()\n",
    "#   pred_90 = model_90.predict(last_sample.values.reshape(1,-1))\n",
    "#   last_sample['t_90'] = pred_90.item()\n",
    "#   pred_120 = model_120.predict(last_sample.values.reshape(1,-1))\n",
    "  \n",
    "#   # fill in test data using above predictions\n",
    "#   test.iloc[set-1,test.columns.get_indexer(['30_min_horizon'])] = np.round(pred_30.item())\n",
    "#   test.iloc[set-1,test.columns.get_indexer(['60_min_horizon'])] = np.round(pred_60.item())\n",
    "#   test.iloc[set-1,test.columns.get_indexer(['90_min_horizon'])] = np.round(pred_90.item())\n",
    "#   test.iloc[set-1,test.columns.get_indexer(['120_min_horizon'])] = np.round(pred_120.item())\n",
    "\n",
    "#   file_count += 1\n",
    "#   if file_count%30 == 0 :\n",
    "#       print(file_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wd_test = pd.read_csv(r'data\\shell_test.csv')\n",
    "test = pd.read_csv(r'data\\test.csv')\n",
    "\n",
    "file_count = 0\n",
    "for set in range(1,301):\n",
    "  wd = wd_test[wd_test['scenario_set'] == set]\n",
    "  # preprocessing test data\n",
    "  last_sample = preprocess(wd,mode='test')\n",
    "\n",
    "  # predicting test samples\n",
    "  pred_30 = cross_val_predict(model_30,last_sample.values.reshape(1,-1))\n",
    "  last_sample['t_30'] = pred_30.item()\n",
    "  pred_60 = cross_val_predict(model_60,last_sample.values.reshape(1,-1))\n",
    "  last_sample['t_60'] = pred_60.item()\n",
    "  pred_90 = cross_val_predict(model_90,last_sample.values.reshape(1,-1))\n",
    "  last_sample['t_90'] = pred_90.item()\n",
    "  pred_120 = cross_val_predict(model_120,last_sample.values.reshape(1,-1))\n",
    "  \n",
    "  # fill in test data using above predictions\n",
    "  test.iloc[set-1,test.columns.get_indexer(['30_min_horizon'])] = np.round(pred_30.item())\n",
    "  test.iloc[set-1,test.columns.get_indexer(['60_min_horizon'])] = np.round(pred_60.item())\n",
    "  test.iloc[set-1,test.columns.get_indexer(['90_min_horizon'])] = np.round(pred_90.item())\n",
    "  test.iloc[set-1,test.columns.get_indexer(['120_min_horizon'])] = np.round(pred_120.item())\n",
    "\n",
    "  file_count += 1\n",
    "  if file_count%30 == 0 :\n",
    "      print(file_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       scenario_set  30_min_horizon  60_min_horizon  90_min_horizon  \\\n",
      "count    300.000000      300.000000      300.000000      300.000000   \n",
      "mean     150.500000       57.243333       57.903333       58.186667   \n",
      "std       86.746758       30.752178       27.706549       24.684060   \n",
      "min        1.000000        6.000000        5.000000        6.000000   \n",
      "25%       75.750000       24.000000       29.750000       36.750000   \n",
      "50%      150.500000       62.500000       66.000000       66.500000   \n",
      "75%      225.250000       86.000000       82.000000       78.000000   \n",
      "max      300.000000       99.000000       98.000000       97.000000   \n",
      "\n",
      "       120_min_horizon  \n",
      "count       300.000000  \n",
      "mean         58.336667  \n",
      "std          21.939793  \n",
      "min           6.000000  \n",
      "25%          42.750000  \n",
      "50%          64.500000  \n",
      "75%          75.000000  \n",
      "max          98.000000  \n",
      "     scenario_set  30_min_horizon  60_min_horizon  90_min_horizon  \\\n",
      "0               1            93.0            94.0            95.0   \n",
      "1               2            53.0            65.0            72.0   \n",
      "2               3            88.0            79.0            64.0   \n",
      "3               4            55.0            61.0            68.0   \n",
      "4               5            24.0            27.0            30.0   \n",
      "..            ...             ...             ...             ...   \n",
      "295           296            11.0            11.0            12.0   \n",
      "296           297            66.0            76.0            73.0   \n",
      "297           298            15.0            20.0            25.0   \n",
      "298           299            41.0            54.0            59.0   \n",
      "299           300            33.0            42.0            46.0   \n",
      "\n",
      "     120_min_horizon  \n",
      "0               91.0  \n",
      "1               67.0  \n",
      "2               52.0  \n",
      "3               73.0  \n",
      "4               36.0  \n",
      "..               ...  \n",
      "295             13.0  \n",
      "296             71.0  \n",
      "297             31.0  \n",
      "298             63.0  \n",
      "299             50.0  \n",
      "\n",
      "[300 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# test = test.applymap(int)\n",
    "test.to_csv('voting_regressor_preds.csv',index=False)\n",
    "\n",
    "print(test.describe())\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "518a807dccee1bb2cf00c0cea9388abbe4210a3c385c01718c979ef7759eaf87"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
